{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW_DgC9MQwOO"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx-XNUfuQ0LD",
        "outputId": "7ad0acfb-3329-4315-9d6f-1b5f6bc5ad58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 01: Load data\n",
        "def load_yaml(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = yaml.safe_load(file)\n",
        "    return data\n",
        "\n"
      ],
      "metadata": {
        "id": "XXuQVkHqRH8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlu_data = load_yaml('nlu.yml')\n",
        "stories_data = load_yaml('stories.yml')\n",
        "rules_data = load_yaml('rules.yml')\n",
        "\n",
        "# Combine the loaded data into a single dictionary\n",
        "data = {'nlu': nlu_data, 'stories': stories_data, 'rules': rules_data}"
      ],
      "metadata": {
        "id": "Mce82EA4RSmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_stories_rules_data(data):\n",
        "    processed_data = {'stories': [], 'rules': []}\n",
        "\n",
        "    # Preprocess Stories\n",
        "    stories = data.get('stories', {}).get('stories', [])\n",
        "    for story_data in stories:\n",
        "        story_name = story_data.get('story', '')\n",
        "        steps = story_data.get('steps', [])\n",
        "        processed_data['stories'].append({'story': story_name, 'steps': steps})\n",
        "\n",
        "    # Preprocess Rules\n",
        "    rules = data.get('rules', {}).get('rules', [])\n",
        "    for rule_data in rules:\n",
        "        rule_name = rule_data.get('rule', '')\n",
        "        steps = rule_data.get('steps', [])\n",
        "        processed_data['rules'].append({'rule': rule_name, 'steps': steps})\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# Get the preprocessed Stories and Rules data\n",
        "train_stories_rules = preprocess_stories_rules_data(data)\n",
        "\n",
        "# Print the preprocessed Stories and Rules data\n",
        "print(train_stories_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvG6qt_XRWR9",
        "outputId": "5a3457f5-b453-4b75-f340-213c179bc481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'stories': [{'story': 'greet', 'steps': [{'intent': 'greet'}, {'action': 'utter_greet'}]}, {'story': 'goodbye', 'steps': [{'intent': 'goodbye'}, {'action': 'utter_goodbye'}]}, {'story': 'places_info', 'steps': [{'intent': 'placesinfo'}, {'action': 'utter_placesinfo'}]}, {'story': 'location', 'steps': [{'intent': 'location'}, {'action': 'utter_location'}]}, {'story': 'categories', 'steps': [{'intent': 'categories'}, {'action': 'utter_categories'}]}, {'story': 'recommendation', 'steps': [{'intent': 'recommendation'}, {'action': 'utter_recommendation'}]}, {'story': 'mood_happy', 'steps': [{'intent': 'mood_happy'}, {'action': 'utter_mood_happy'}]}, {'story': 'mood_unhappy', 'steps': [{'intent': 'mood_unhappy'}, {'action': 'utter_mood_unhappy'}]}, {'story': 'mood_sad', 'steps': [{'intent': 'mood_sad'}, {'action': 'utter_mood_sad'}]}, {'story': 'bot_challenge', 'steps': [{'intent': 'bot_challenge'}, {'action': 'utter_bot_challenge'}]}, {'story': 'happy', 'steps': [{'intent': 'happy'}, {'action': 'utter_happy'}]}], 'rules': [{'rule': 'Handle greet intent', 'steps': [{'intent': 'greet'}, {'action': 'utter_greet'}]}, {'rule': 'Handle goodbye intent', 'steps': [{'intent': 'goodbye'}, {'action': 'utter_goodbye'}]}, {'rule': 'Handle placesinfo intent', 'steps': [{'intent': 'placesinfo'}, {'action': 'utter_placesinfo'}]}, {'rule': 'Handle location intent', 'steps': [{'intent': 'location'}, {'action': 'utter_location'}]}, {'rule': 'Handle categories intent', 'steps': [{'intent': 'categories'}, {'action': 'utter_categories'}]}, {'rule': 'Handle recommendation intent', 'steps': [{'intent': 'recommendation'}, {'action': 'utter_recommendation'}]}, {'rule': 'Handle mood_happy intent', 'steps': [{'intent': 'mood_happy'}, {'action': 'utter_mood_happy'}]}, {'rule': 'Handle mood_unhappy intent', 'steps': [{'intent': 'mood_unhappy'}, {'action': 'utter_mood_unhappy'}]}, {'rule': 'Handle mood_sad intent', 'steps': [{'intent': 'mood_sad'}, {'action': 'utter_mood_sad'}]}, {'rule': 'Handle bot_challenge intent', 'steps': [{'intent': 'bot_challenge'}, {'action': 'utter_bot_challenge'}]}, {'rule': 'Handle happy intent', 'steps': [{'intent': 'happy'}, {'action': 'utter_happy'}]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_nlu_data(data):\n",
        "    processed_data = {'nlu': []}\n",
        "\n",
        "    # Preprocess NLU data\n",
        "    nlu_intents = data.get('nlu', {}).get('intents', [])\n",
        "    for intent_data in nlu_intents:\n",
        "        intent_name = intent_data.get('intent', '')\n",
        "        examples = intent_data.get('examples', [])\n",
        "        processed_data['nlu'].append({'intent': intent_name, 'examples': examples})\n",
        "\n",
        "    return processed_data\n"
      ],
      "metadata": {
        "id": "Oa-AUfpKRk6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_dialogue_training_data(stories_rules_data):\n",
        "    dialogue_data = {'user_input': [], 'bot_response': []}\n",
        "\n",
        "    # Extract user input and bot response pairs from stories\n",
        "    for story_data in stories_rules_data.get('stories', []):\n",
        "        steps = story_data.get('steps', [])\n",
        "        user_turns = [step.get('intent', '') for step in steps if 'intent' in step]\n",
        "        bot_turns = [step.get('action', '') for step in steps if 'action' in step]\n",
        "\n",
        "        # Exclude empty turns\n",
        "        user_turns = [turn for turn in user_turns if turn]\n",
        "        bot_turns = [turn for turn in bot_turns if turn]\n",
        "\n",
        "        # Combine user and bot turns into dialogue pairs\n",
        "        dialogue_pairs = list(zip(user_turns, bot_turns))\n",
        "\n",
        "        # Add the dialogue pairs to the training data\n",
        "        dialogue_data['user_input'].extend([pair[0] for pair in dialogue_pairs])\n",
        "        dialogue_data['bot_response'].extend([pair[1] for pair in dialogue_pairs])\n",
        "\n",
        "    return dialogue_data\n",
        "\n",
        "# Get the dialogue training data\n",
        "train_dialogue_data = extract_dialogue_training_data(train_stories_rules)\n",
        "\n",
        "# Print the dialogue training data\n",
        "print(train_dialogue_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGHmvX3-aQUs",
        "outputId": "6e8542ef-ff9d-4b15-9648-7ce936e115d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'user_input': ['greet', 'goodbye', 'placesinfo', 'location', 'categories', 'recommendation', 'mood_happy', 'mood_unhappy', 'mood_sad', 'bot_challenge', 'happy'], 'bot_response': ['utter_greet', 'utter_goodbye', 'utter_placesinfo', 'utter_location', 'utter_categories', 'utter_recommendation', 'utter_mood_happy', 'utter_mood_unhappy', 'utter_mood_sad', 'utter_bot_challenge', 'utter_happy']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def encode_dialogue_data(user_input, bot_response):\n",
        "    # Combine user input and bot response into a single list\n",
        "    dialogue_corpus = user_input + bot_response\n",
        "\n",
        "    # Use CountVectorizer to convert text data into numerical vectors\n",
        "    vectorizer = CountVectorizer()\n",
        "    dialogue_vectors = vectorizer.fit_transform(dialogue_corpus).toarray()\n",
        "\n",
        "    # Separate the vectors back into user input and bot response\n",
        "    user_input_vectors = dialogue_vectors[:len(user_input)]\n",
        "    bot_response_vectors = dialogue_vectors[len(user_input):]\n",
        "\n",
        "    return user_input_vectors, bot_response_vectors\n",
        "\n",
        "# Encode the dialogue training data\n",
        "train_user_input_vectors, train_bot_response_vectors = encode_dialogue_data(\n",
        "    train_dialogue_data['user_input'],\n",
        "    train_dialogue_data['bot_response']\n",
        ")\n",
        "\n",
        "# Print the encoded dialogue training data\n",
        "print(\"User Input Vectors:\")\n",
        "print(train_user_input_vectors)\n",
        "print(\"\\nBot Response Vectors:\")\n",
        "print(train_bot_response_vectors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04MV_p3fZxuo",
        "outputId": "1a3e2999-0e0f-4e6a-dc4a-25652a15d441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Input Vectors:\n",
            "[[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            "Bot Response Vectors:\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NRx4Od4gZx__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add a new special token for padding\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Set the new padding token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Example: Fine-tune the model on your dialogue data\n",
        "# Replace this with your actual dialogue data\n",
        "dialogue_data = [\n",
        "    \"User: How are you?\",\n",
        "    \"Bot: I'm good, thanks!\",\n",
        "    \"User: Tell me a joke.\",\n",
        "    \"Bot: Why don't scientists trust atoms? Because they make up everything!\"\n",
        "]\n",
        "\n",
        "# Save the dialogue data to a text file\n",
        "with open(\"dialogue_data.txt\", \"w\") as file:\n",
        "    file.write(\"\\n\".join(dialogue_data))\n",
        "\n",
        "# Tokenize the dialogue data with padding and truncation\n",
        "tokenized_data = tokenizer(dialogue_data, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "\n",
        "# Create a TextDataset for training\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"dialogue_data.txt\",  # Use the path to the text file\n",
        "    block_size=128,  # Adjust based on your data and available resources\n",
        "    overwrite_cache=True,\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=1,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the GPT-2 model on your data\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2-finetuned\")\n"
      ],
      "metadata": {
        "id": "MPLxYRolSWoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wrbkts18U1MO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}